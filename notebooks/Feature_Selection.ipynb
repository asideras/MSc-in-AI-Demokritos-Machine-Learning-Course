{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feature_extraction_ML as fe\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Load murmur_dataset.csv (contains all features that have been extracted).\n",
    "2) Get the augmented samples to a different dataframe.\n",
    "3) Drop 'raw' audio\n",
    "4) There are very few nan and +/- inf values, replace them with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes' distribution:\n",
      "MURMUR\n",
      "Absent     457\n",
      "Present    301\n",
      "Name: Patient_ID, dtype: int64\n",
      "dataframe without duplicate samples shape: (482, 134)\n",
      "augmented positive samples dataframe shape: (276, 134)\n"
     ]
    }
   ],
   "source": [
    "#data = pd.read_csv('../murmor_dataset.csv') \n",
    "data = pd.read_csv('./murmor_dataset.csv') \n",
    "\n",
    "print(f\"Classes' distribution:\")\n",
    "print(data.groupby('MURMUR').count()['Patient_ID'])\n",
    "duplicates = data[data.duplicated(['Patient_ID'],keep=False)]\n",
    "duplicates = duplicates.sort_values(by=['Patient_ID'])\n",
    "data.drop_duplicates(subset=['Patient_ID'],keep=False, inplace=True)\n",
    "print(f'dataframe without duplicate samples shape: {data.shape}')\n",
    "print(f'augmented positive samples dataframe shape: {duplicates.shape}')\n",
    "y = data.MURMUR\n",
    "y = y.replace({'Present':1,'Absent':0})\n",
    "data = data.drop(columns=['Patient_ID', 'AV', 'MV', 'PV', 'TV','MURMUR'])\n",
    "data = data.fillna(0)\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split non-augmented data to 3 datasets. Training, validation and test sets.\n",
    "We will use the training set in order to fit our classifiers.\n",
    "We will use the validation set for hyperparameter tuning.\n",
    "The test set will be used for the final evaluation of our hypothesis. \n",
    "We won't make any choice about our classifier or the features will be used using this set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(data, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size=0.25, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size = (288, 128) , 0.6 %\n",
      "Validation dataset size = (97, 128)  , 0.2 %\n",
      "Test dataset size = (97, 128) , 0.2 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Training dataset size = {X_train.shape} , {round(X_train.shape[0]/data.shape[0], 2)} %')\n",
    "print(f'Validation dataset size = {X_val.shape}  , {round(X_val.shape[0]/data.shape[0] , 2)} %')\n",
    "print(f'Test dataset size = {X_test.shape} , {round(X_test.shape[0]/data.shape[0], 2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to distribute the augmented samples to the 3 datasets. We are doing so, in order to make sure that samples coming from the same patients have been assigned ton the same dataset. Full explanation can be found in the report. By using the split_augmented_samples() method we define also the percentage of positive samples we would like to add to each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0 unique patients' samples have been duplicated, by a factor of 3\n",
      "74 unique patients' samples to add to training dataset\n",
      "9 unique patients' samples to add to validation dataset\n",
      "9 unique patients' samples to add to testing dataset\n"
     ]
    }
   ],
   "source": [
    "X_to_train,y_to_train, X_to_val, y_to_val, X_to_test, y_to_test = fe.split_augmented_samples(duplicates,train_percentage=.8, validation_percentage=.1, test_percentage=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.append(X_to_train)\n",
    "y_train = y_train.append(y_to_train)\n",
    "\n",
    "X_val = X_val.append(X_to_val)\n",
    "y_val = y_val.append(y_to_val)\n",
    "\n",
    "X_test = X_test.append(X_to_test)\n",
    "y_test = y_test.append(y_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples : 758\n",
      "final training set size : 510 samples, 67.28 %\n",
      "final validation set size : 124 samples, 16.36 %\n",
      "final testing set size : 124 samples, 16.36 %\n"
     ]
    }
   ],
   "source": [
    "total = X_train.shape[0] + X_val.shape[0] + X_test.shape[0]\n",
    "print(f'total samples : {total}')\n",
    "print(f'final training set size : {X_train.shape[0]} samples, {round(X_train.shape[0]/total,4) * 100} %')\n",
    "print(f'final validation set size : {X_val.shape[0]} samples, {round(X_val.shape[0]/total,4) * 100} %')\n",
    "print(f'final testing set size : {X_test.shape[0]} samples, {round(X_test.shape[0]/total,4) * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive sample rate in training set : 0.47\n",
      "positive sample rate in validation set : 0.27\n",
      "positive sample rate in testing set : 0.24\n"
     ]
    }
   ],
   "source": [
    "print(f'positive sample rate in training set : {round(y_train.sum()/y_train.shape[0],2)}')\n",
    "print(f'positive sample rate in validation set : {round(y_val.sum()/y_val.shape[0],2)}')\n",
    "print(f'positive sample rate in testing set : {round(y_test.sum()/y_test.shape[0], 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"../train_val_test_datasets/X_train.csv\", index=False)\n",
    "X_val.to_csv(\"../train_val_test_datasets/X_val.csv\", index=False)\n",
    "X_test.to_csv(\"../train_val_test_datasets/X_test.csv\", index=False)\n",
    "\n",
    "\n",
    "y_train.to_csv(\"../train_val_test_datasets/y_train.csv\", index=False)\n",
    "y_val.to_csv(\"../train_val_test_datasets/y_val.csv\", index=False)\n",
    "y_test.to_csv(\"../train_val_test_datasets/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection using Lasso Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine training and validation sets and use Cross Validation technique in the grid searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.append(X_val)\n",
    "y_train = y_train.append(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logistic__C': 100}\n",
      "lasso regression performed one kind of feature elimination (=0 value on the coefficients) on 1 features\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "logistic = LogisticRegression(max_iter=100000, penalty= 'l1', solver='liblinear')\n",
    "pipe = Pipeline(steps=[(\"scaler\", scaler), (\"logistic\", logistic)])\n",
    "param_grid = {\n",
    "    \"logistic__C\": [0.1,0.2,0.6,1,1.6,2,4,10,20,100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe,  \n",
    "                           param_grid,\n",
    "                           scoring = 'f1',\n",
    "                           cv = 4,\n",
    "                           verbose=0)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)   \n",
    "print(grid_search.best_params_)\n",
    "coefficients = grid_search.best_estimator_.named_steps['logistic'].coef_[0]\n",
    "importance = np.abs(coefficients)\n",
    "print(f'lasso regression performed one kind of feature elimination (=0 value on the coefficients) on {np.array(data.columns)[importance == 0].shape[0]} features')\n",
    "feat_importances = pd.Series(importance)\n",
    "feat_importances = feat_importances[feat_importances >0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the selected features to a .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 features have been selected\n"
     ]
    }
   ],
   "source": [
    "imp_features = feat_importances.to_dict()\n",
    "columns = data.columns\n",
    "result = []\n",
    "new_columns = []\n",
    "for col, value in imp_features.items():\n",
    "    result.append((columns[col],value))\n",
    "    new_columns.append(columns[col])\n",
    "print(f'{feat_importances.shape[0]} features have been selected')\n",
    "with open(r'../important_features/logistic_regression_lasso.txt', 'w') as fp:\n",
    "    for item in new_columns:\n",
    "        fp.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Feature Selection using ANOVA F-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k=94)\n",
    "selector.fit(X_train,y_train)\n",
    "anova_result = selector.get_feature_names_out(columns)\n",
    "with open(r'../important_features/anova.txt', 'w') as fp:\n",
    "    for item in anova_result:\n",
    "        fp.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "min_features_to_select = 20  # Minimum number of features to consider\n",
    "clf = LogisticRegression()\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=2,\n",
    ")\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = rfecv.get_feature_names_out(columns)\n",
    "with open(r'../important_features/rfe.txt', 'w') as fp:\n",
    "    for item in rfe:\n",
    "        fp.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
