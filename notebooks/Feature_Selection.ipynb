{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feature_extraction_ML as fe\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Load murmur_dataset.csv (contains all features that have been extracted).\n",
    "2) Get the augmented samples to a different dataframe.\n",
    "3) Drop 'raw' audio\n",
    "4) There are very few nan and +/- inf values, replace them with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes' distribution:\n",
      "MURMUR\n",
      "Absent     457\n",
      "Present    203\n",
      "Name: Patient_ID, dtype: int64\n",
      "dataframe without duplicate samples shape: (488, 134)\n",
      "augmented positive samples dataframe shape: (172, 134)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../murmor_dataset.csv') \n",
    "\n",
    "print(f\"Classes' distribution:\")\n",
    "print(data.groupby('MURMUR').count()['Patient_ID'])\n",
    "duplicates = data[data.duplicated(['Patient_ID'],keep=False)]\n",
    "duplicates = duplicates.sort_values(by=['Patient_ID'])\n",
    "data.drop_duplicates(subset=['Patient_ID'],keep=False, inplace=True)\n",
    "print(f'dataframe without duplicate samples shape: {data.shape}')\n",
    "print(f'augmented positive samples dataframe shape: {duplicates.shape}')\n",
    "y = data.MURMUR\n",
    "y = y.replace({'Present':1,'Absent':0})\n",
    "data = data.drop(columns=['Patient_ID', 'AV', 'MV', 'PV', 'TV','MURMUR'])\n",
    "data = data.fillna(0)\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split non-augmented data to 3 datasets. Training, validation and test sets.\n",
    "We will use the training set in order to fit our classifiers.\n",
    "We will use the validation set for hyperparameter tuning.\n",
    "The test set will be used for the final evaluation of our hypothesis. \n",
    "We won't make any choice about our classifier or the features will be used using this set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(data, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size=0.25, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size = (292, 128) , 0.6 %\n",
      "Validation dataset size = (98, 128)  , 0.2 %\n",
      "Test dataset size = (98, 128) , 0.2 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Training dataset size = {X_train.shape} , {round(X_train.shape[0]/data.shape[0], 2)} %')\n",
    "print(f'Validation dataset size = {X_val.shape}  , {round(X_val.shape[0]/data.shape[0] , 2)} %')\n",
    "print(f'Test dataset size = {X_test.shape} , {round(X_test.shape[0]/data.shape[0], 2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to distribute the augmented samples to the 3 datasets. We are doing so, in order to make sure that samples coming from the same patients have been assigned ton the same dataset. Full explanation can be found in the report. By using the split_augmented_samples() method we define also the percentage of positive samples we would like to add to each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.0 unique patients' samples have been duplicated, by a factor of 2\n",
      "70 unique patients' samples to add to training dataset\n",
      "8 unique patients' samples to add to validation dataset\n",
      "8 unique patients' samples to add to testing dataset\n"
     ]
    }
   ],
   "source": [
    "X_to_train,y_to_train, X_to_val, y_to_val, X_to_test, y_to_test = fe.split_augmented_samples(duplicates,train_percentage=.8, validation_percentage=.1, test_percentage=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.append(X_to_train)\n",
    "y_train = y_train.append(y_to_train)\n",
    "\n",
    "X_val = X_val.append(X_to_val)\n",
    "y_val = y_val.append(y_to_val)\n",
    "\n",
    "X_test = X_test.append(X_to_test)\n",
    "y_test = y_test.append(y_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples : 660\n",
      "final training set size : 432 samples, 65.45 %\n",
      "final validation set size : 114 samples, 17.27 %\n",
      "final testing set size : 114 samples, 17.27 %\n"
     ]
    }
   ],
   "source": [
    "total = X_train.shape[0] + X_val.shape[0] + X_test.shape[0]\n",
    "print(f'total samples : {total}')\n",
    "print(f'final training set size : {X_train.shape[0]} samples, {round(X_train.shape[0]/total,4) * 100} %')\n",
    "print(f'final validation set size : {X_val.shape[0]} samples, {round(X_val.shape[0]/total,4) * 100} %')\n",
    "print(f'final testing set size : {X_test.shape[0]} samples, {round(X_test.shape[0]/total,4) * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive sample rate in training set : 0.36\n",
      "positive sample rate in validation set : 0.18\n",
      "positive sample rate in testing set : 0.23\n"
     ]
    }
   ],
   "source": [
    "print(f'positive sample rate in training set : {round(y_train.sum()/y_train.shape[0],2)}')\n",
    "print(f'positive sample rate in validation set : {round(y_val.sum()/y_val.shape[0],2)}')\n",
    "print(f'positive sample rate in testing set : {round(y_test.sum()/y_test.shape[0], 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"../train_val_test_datasets/X_train.csv\", index=False)\n",
    "X_val.to_csv(\"../train_val_test_datasets/X_val.csv\", index=False)\n",
    "X_test.to_csv(\"../train_val_test_datasets/X_test.csv\", index=False)\n",
    "\n",
    "\n",
    "y_train.to_csv(\"../train_val_test_datasets/y_train.csv\", index=False)\n",
    "y_val.to_csv(\"../train_val_test_datasets/y_val.csv\", index=False)\n",
    "y_test.to_csv(\"../train_val_test_datasets/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection using Lasso Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine training and validation sets and use Cross Validation technique in the grid searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.append(X_val)\n",
    "y_train = y_train.append(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logistic__C': 3}\n",
      "lasso regression performed one kind of feature elimination (=0 value on the coefficients) on 34 features\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "logistic = LogisticRegression(max_iter=100000, penalty= 'l1', solver='liblinear')\n",
    "pipe = Pipeline(steps=[(\"scaler\", scaler), (\"logistic\", logistic)])\n",
    "param_grid = {\n",
    "    \"logistic__C\": [0.1,0.2,0.6,1,1.6,2,4,10,20,100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe,  \n",
    "                           param_grid,\n",
    "                           scoring = 'f1',\n",
    "                           cv = 4,\n",
    "                           verbose=0)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)   \n",
    "print(grid_search.best_params_)\n",
    "coefficients = grid_search.best_estimator_.named_steps['logistic'].coef_[0]\n",
    "importance = np.abs(coefficients)\n",
    "print(f'lasso regression performed one kind of feature elimination (=0 value on the coefficients) on {np.array(data.columns)[importance == 0].shape[0]} features')\n",
    "feat_importances = pd.Series(importance)\n",
    "feat_importances = feat_importances[feat_importances >0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the selected features to a .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 features have been selected\n"
     ]
    }
   ],
   "source": [
    "imp_features = feat_importances.to_dict()\n",
    "columns = data.columns\n",
    "result = []\n",
    "new_columns = []\n",
    "for col, value in imp_features.items():\n",
    "    result.append((columns[col],value))\n",
    "    new_columns.append(columns[col])\n",
    "print(f'{feat_importances.shape[0]} features have been selected')\n",
    "with open(r'../important_features/logistic_regression_lasso.txt', 'w') as fp:\n",
    "    for item in new_columns:\n",
    "        fp.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Feature Selection using ANOVA F-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SelectKBest(k=94)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectKBest</label><div class=\"sk-toggleable__content\"><pre>SelectKBest(k=94)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SelectKBest(k=94)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectKBest(f_classif, k=94)\n",
    "selector.fit(X_train,y_train)\n",
    "anova_result = selector.get_feature_names_out(columns)\n",
    "with open(r'../important_features/anova.txt', 'w') as fp:\n",
    "    for item in anova_result:\n",
    "        fp.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "min_features_to_select = 20  # Minimum number of features to consider\n",
    "clf = LogisticRegression()\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=2,\n",
    ")\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = rfecv.get_feature_names_out(columns)\n",
    "with open(r'../important_features/rfe.txt', 'w') as fp:\n",
    "    for item in rfe:\n",
    "        fp.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
