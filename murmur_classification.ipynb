{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f644ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e0569",
   "metadata": {},
   "source": [
    "1) data loading (final features)\n",
    "2) get the augmented samples to different dataframe\n",
    "3) get label column to different pandas series\n",
    "4) drop 'raw' audio\n",
    "5) there are very few nan and +/- inf values, replace them with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8062f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes' distribution:\n",
      "MURMUR\n",
      "Absent     457\n",
      "Present    203\n",
      "Name: Patient_ID, dtype: int64\n",
      "dataframe without duplicate samples shape: (488, 134)\n",
      "augmented positive samples dataframe shape: (172, 134)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('murmor_dataset.csv') \n",
    "print(f\"Classes' distribution:\")\n",
    "print(data.groupby('MURMUR').count()['Patient_ID'])\n",
    "duplicates = data[data.duplicated(['Patient_ID'],keep=False)]\n",
    "duplicates = duplicates.sort_values(by=['Patient_ID'])\n",
    "data.drop_duplicates(subset=['Patient_ID'],keep=False, inplace=True)\n",
    "print(f'dataframe without duplicate samples shape: {data.shape}')\n",
    "print(f'augmented positive samples dataframe shape: {duplicates.shape}')\n",
    "y = data.MURMUR\n",
    "y = y.replace({'Present':1,'Absent':0})\n",
    "data = data.drop(columns=['Patient_ID', 'AV', 'MV', 'PV', 'TV','MURMUR'])\n",
    "data = data.fillna(0)\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c440d5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_ae_AV</th>\n",
       "      <th>mean_ae_MV</th>\n",
       "      <th>mean_ae_PV</th>\n",
       "      <th>mean_ae_TV</th>\n",
       "      <th>median_ae_AV</th>\n",
       "      <th>median_ae_MV</th>\n",
       "      <th>median_ae_PV</th>\n",
       "      <th>median_ae_TV</th>\n",
       "      <th>std_ae_AV</th>\n",
       "      <th>std_ae_MV</th>\n",
       "      <th>...</th>\n",
       "      <th>TV_mfcc_4</th>\n",
       "      <th>TV_mfcc_5</th>\n",
       "      <th>TV_mfcc_6</th>\n",
       "      <th>TV_mfcc_7</th>\n",
       "      <th>TV_mfcc_8</th>\n",
       "      <th>TV_mfcc_9</th>\n",
       "      <th>TV_mfcc_10</th>\n",
       "      <th>TV_mfcc_11</th>\n",
       "      <th>TV_mfcc_12</th>\n",
       "      <th>TV_mfcc_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.093476</td>\n",
       "      <td>0.083762</td>\n",
       "      <td>0.164984</td>\n",
       "      <td>0.107563</td>\n",
       "      <td>0.047040</td>\n",
       "      <td>0.033079</td>\n",
       "      <td>0.087165</td>\n",
       "      <td>0.042905</td>\n",
       "      <td>0.102132</td>\n",
       "      <td>0.131644</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.460999</td>\n",
       "      <td>-6.598893</td>\n",
       "      <td>34.000908</td>\n",
       "      <td>21.154072</td>\n",
       "      <td>-7.830566</td>\n",
       "      <td>-1.645628</td>\n",
       "      <td>21.914124</td>\n",
       "      <td>20.209282</td>\n",
       "      <td>-0.958162</td>\n",
       "      <td>-6.293470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.091702</td>\n",
       "      <td>0.099159</td>\n",
       "      <td>0.121979</td>\n",
       "      <td>0.129162</td>\n",
       "      <td>0.059809</td>\n",
       "      <td>0.067497</td>\n",
       "      <td>0.076158</td>\n",
       "      <td>0.042184</td>\n",
       "      <td>0.085225</td>\n",
       "      <td>0.115260</td>\n",
       "      <td>...</td>\n",
       "      <td>-29.298609</td>\n",
       "      <td>-5.337496</td>\n",
       "      <td>30.698105</td>\n",
       "      <td>20.061325</td>\n",
       "      <td>-6.931499</td>\n",
       "      <td>-4.781078</td>\n",
       "      <td>14.501362</td>\n",
       "      <td>14.630821</td>\n",
       "      <td>-2.321874</td>\n",
       "      <td>-7.382983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.125086</td>\n",
       "      <td>0.189740</td>\n",
       "      <td>0.169284</td>\n",
       "      <td>0.204370</td>\n",
       "      <td>0.089281</td>\n",
       "      <td>0.112374</td>\n",
       "      <td>0.132720</td>\n",
       "      <td>0.140140</td>\n",
       "      <td>0.105365</td>\n",
       "      <td>0.181673</td>\n",
       "      <td>...</td>\n",
       "      <td>-42.962135</td>\n",
       "      <td>-18.818758</td>\n",
       "      <td>24.844090</td>\n",
       "      <td>18.756638</td>\n",
       "      <td>-8.074683</td>\n",
       "      <td>-5.306619</td>\n",
       "      <td>16.791115</td>\n",
       "      <td>18.958763</td>\n",
       "      <td>1.252272</td>\n",
       "      <td>-5.608494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_ae_AV  mean_ae_MV  mean_ae_PV  mean_ae_TV  median_ae_AV  median_ae_MV  \\\n",
       "0    0.093476    0.083762    0.164984    0.107563      0.047040      0.033079   \n",
       "2    0.091702    0.099159    0.121979    0.129162      0.059809      0.067497   \n",
       "4    0.125086    0.189740    0.169284    0.204370      0.089281      0.112374   \n",
       "\n",
       "   median_ae_PV  median_ae_TV  std_ae_AV  std_ae_MV  ...  TV_mfcc_4  \\\n",
       "0      0.087165      0.042905   0.102132   0.131644  ... -33.460999   \n",
       "2      0.076158      0.042184   0.085225   0.115260  ... -29.298609   \n",
       "4      0.132720      0.140140   0.105365   0.181673  ... -42.962135   \n",
       "\n",
       "   TV_mfcc_5  TV_mfcc_6  TV_mfcc_7  TV_mfcc_8  TV_mfcc_9  TV_mfcc_10  \\\n",
       "0  -6.598893  34.000908  21.154072  -7.830566  -1.645628   21.914124   \n",
       "2  -5.337496  30.698105  20.061325  -6.931499  -4.781078   14.501362   \n",
       "4 -18.818758  24.844090  18.756638  -8.074683  -5.306619   16.791115   \n",
       "\n",
       "   TV_mfcc_11  TV_mfcc_12  TV_mfcc_13  \n",
       "0   20.209282   -0.958162   -6.293470  \n",
       "2   14.630821   -2.321874   -7.382983  \n",
       "4   18.958763    1.252272   -5.608494  \n",
       "\n",
       "[3 rows x 128 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4c982",
   "metadata": {},
   "source": [
    "# Transformation - Data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6634808",
   "metadata": {},
   "source": [
    "Split non-augmented data to 3 datasets. Training, validation and test sets.\n",
    "We will use the training set in order to fit our classifiers.\n",
    "We will use the validation set for hyperparameter tuning.\n",
    "The test set will be used for the final evaluation of our hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042bfe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(data, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70528d1f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size = (292, 128) , 0.6 %\n",
      "Validation dataset size = (98, 128)  , 0.2 %\n",
      "Test dataset size = (98, 128) , 0.2 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Training dataset size = {X_train.shape} , {round(X_train.shape[0]/data.shape[0], 2)} %')\n",
    "print(f'Validation dataset size = {X_val.shape}  , {round(X_val.shape[0]/data.shape[0] , 2)} %')\n",
    "print(f'Test dataset size = {X_test.shape} , {round(X_test.shape[0]/data.shape[0], 2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f031f752",
   "metadata": {},
   "source": [
    "Now, we need to distribute the augmented samples to the 3 datasets. We are doing so, in order to make sure that samples coming from the same patients have been assigned ton the same dataset. Full explanation can be found in the report. By using the split_augmented_samples() method we define also the percentage of positive samples we whould like to add to each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88e637c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172, 134)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16e12f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_augmented_samples(data, train_percentage = .8, validation_percentage = .1, test_percentage = .1 , duplicate_factor = 2):\n",
    "    duplicates = data.sort_values(by=['Patient_ID'])\n",
    "    num_of_diff_samples = duplicates.shape[0] / duplicate_factor\n",
    "    \n",
    "    print(f'{num_of_diff_samples} unique samples have been duplicated, by a factor of {duplicate_factor}')\n",
    "    \n",
    "    trains_added = int(train_percentage * num_of_diff_samples) \n",
    "    vals_added = int(validation_percentage * num_of_diff_samples)\n",
    "    tests_added = int(test_percentage * num_of_diff_samples)\n",
    "    residual = int(num_of_diff_samples - trains_added - vals_added - tests_added)\n",
    "    trains_added += residual\n",
    "    \n",
    "    print(f'{trains_added} samples to add to training dataset')\n",
    "    print(f'{vals_added} samples to add to validation dataset')\n",
    "    print(f'{tests_added} samples to add to testing dataset')\n",
    "    \n",
    "    \n",
    "    modified = 0\n",
    "    \n",
    "    while trains_added % duplicate_factor != 0:\n",
    "        trains_added -= 1\n",
    "        modified += 1\n",
    "        \n",
    "    while vals_added % duplicate_factor != 0:\n",
    "        vals_added -= 1\n",
    "        modified += 1\n",
    "    \n",
    "    trains_added += modified\n",
    "    \n",
    "    to_train = duplicates.iloc[:trains_added*duplicate_factor,:]\n",
    "    to_val = duplicates.iloc[trains_added*duplicate_factor:trains_added*duplicate_factor + vals_added*duplicate_factor,:]\n",
    "    to_test = duplicates.iloc[trains_added*duplicate_factor + vals_added*duplicate_factor:,:]\n",
    "    \n",
    "    if to_train.shape[0] % duplicate_factor !=0 or to_val.shape[0] % duplicate_factor !=0 or to_test.shape[0] % duplicate_factor !=0:\n",
    "        raise ValueError\n",
    "    \n",
    "    train_IDs = set(to_train.Patient_ID.unique())\n",
    "    val_IDs = set(to_val.Patient_ID.unique())\n",
    "    test_IDs = set(to_test.Patient_ID.unique())\n",
    "    \n",
    "    \n",
    "    if len(train_IDs.intersection(val_IDs).intersection(test_IDs)) != 0:\n",
    "        raise ValueError\n",
    "        \n",
    "    y_to_train = to_train.MURMUR\n",
    "    y_to_train = y_to_train.replace({'Present':1,'Absent':0})\n",
    "    X_to_train = to_train.drop(columns=['Patient_ID', 'AV', 'MV', 'PV', 'TV','MURMUR'])\n",
    "    X_to_train = X_to_train.fillna(0)\n",
    "    X_to_train.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "\n",
    "    y_to_val = to_val.MURMUR\n",
    "    y_to_val = y_to_val.replace({'Present':1,'Absent':0})\n",
    "    X_to_val = to_val.drop(columns=['Patient_ID', 'AV', 'MV', 'PV', 'TV','MURMUR'])\n",
    "    X_to_val = X_to_val.fillna(0)\n",
    "    X_to_val.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    y_to_test = to_test.MURMUR\n",
    "    y_to_test = y_to_test.replace({'Present':1,'Absent':0})\n",
    "    X_to_test = to_test.drop(columns=['Patient_ID', 'AV', 'MV', 'PV', 'TV','MURMUR'])\n",
    "    X_to_test = X_to_test.fillna(0)\n",
    "    X_to_test.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    \n",
    "    return X_to_train,y_to_train, X_to_val, y_to_val, X_to_test, y_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4118d88f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.0 unique samples have been duplicated, by a factor of 2\n",
      "70 samples to add to training dataset\n",
      "8 samples to add to validation dataset\n",
      "8 samples to add to testing dataset\n"
     ]
    }
   ],
   "source": [
    "X_to_train,y_to_train, X_to_val, y_to_val, X_to_test, y_to_test = split_augmented_samples(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe7316d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_train,y_train,  X_val,y_val ,   X_test, y_test #original\n",
    "X_train = X_train.append(X_to_train)\n",
    "y_train = y_train.append(y_to_train)\n",
    "\n",
    "X_val = X_val.append(X_to_val)\n",
    "y_val = y_val.append(y_to_val)\n",
    "\n",
    "X_test = X_test.append(X_to_test)\n",
    "y_test = y_test.append(y_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45933647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples : 660\n",
      "final training set size : 432 samples, 65.45 %\n",
      "final validation set size : 114 samples, 17.27 %\n",
      "final testing set size : 114 samples, 17.27 %\n"
     ]
    }
   ],
   "source": [
    "total = X_train.shape[0] + X_val.shape[0] + X_test.shape[0]\n",
    "print(f'total samples : {total}')\n",
    "print(f'final training set size : {X_train.shape[0]} samples, {round(X_train.shape[0]/total,4) * 100} %')\n",
    "print(f'final validation set size : {X_val.shape[0]} samples, {round(X_val.shape[0]/total,4) * 100} %')\n",
    "print(f'final testing set size : {X_test.shape[0]} samples, {round(X_test.shape[0]/total,4) * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ba508cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive sample rate in training set : 0.36\n",
      "positive sample rate in validation set : 0.18\n",
      "positive sample rate in testing set : 0.23\n"
     ]
    }
   ],
   "source": [
    "print(f'positive sample rate in training set : {round(y_train.sum()/y_train.shape[0],2)}')\n",
    "print(f'positive sample rate in validation set : {round(y_val.sum()/y_val.shape[0],2)}')\n",
    "print(f'positive sample rate in testing set : {round(y_test.sum()/y_test.shape[0], 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d8e73",
   "metadata": {},
   "source": [
    "# Testing against validation set only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f488896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "s = scaler.fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f0188e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_val,y_pred):\n",
    "    print(f'Precision: {precision_score(y_val,y_pred)}')\n",
    "    print(f'Recall: {recall_score(y_val,y_pred)}')\n",
    "    print(f'f1_score: {f1_score(y_val,y_pred)}')\n",
    "    print(f'Accuracy: {accuracy_score(y_val,y_pred)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95a30b0",
   "metadata": {},
   "source": [
    "# Logistic Regression Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a573ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4444444444444444\n",
      "Recall: 0.5714285714285714\n",
      "f1_score: 0.5\n",
      "Accuracy: 0.7894736842105263\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0,max_iter=1000).fit(X_train, y_train) \n",
    "y_pred = clf.predict(X_val)\n",
    "print_metrics(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c1ff91",
   "metadata": {},
   "source": [
    "# SVM test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d03764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7142857142857143\n",
      "Recall: 0.47619047619047616\n",
      "f1_score: 0.5714285714285714\n",
      "Accuracy: 0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "clf1 = svm.SVC()\n",
    "clf1.fit(X_train, y_train)\n",
    "y_pred = clf1.predict(X_val)\n",
    "print_metrics(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd1153",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62a9f712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.42105263157894735\n",
      "Recall: 0.38095238095238093\n",
      "f1_score: 0.4\n",
      "Accuracy: 0.7894736842105263\n"
     ]
    }
   ],
   "source": [
    "clf2 = GaussianNB()\n",
    "clf2.fit(X_train, y_train)\n",
    "y_pred = clf2.predict(X_val)\n",
    "print_metrics(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb9e692",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f901ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2222222222222222\n",
      "Recall: 0.2857142857142857\n",
      "f1_score: 0.25\n",
      "Accuracy: 0.6842105263157895\n"
     ]
    }
   ],
   "source": [
    "clf3 = KNeighborsClassifier(n_neighbors=3)\n",
    "clf3.fit(X_train, y_train)\n",
    "y_pred = clf3.predict(X_val)\n",
    "print_metrics(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751be154",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a280bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.21212121212121213\n",
      "Recall: 0.3333333333333333\n",
      "f1_score: 0.25925925925925924\n",
      "Accuracy: 0.6491228070175439\n"
     ]
    }
   ],
   "source": [
    "clf4 = tree.DecisionTreeClassifier()\n",
    "clf4.fit(X_train, y_train)\n",
    "y_pred = clf4.predict(X_val)\n",
    "print_metrics(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d457b0c6",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74f247ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.52\n",
      "Recall: 0.6190476190476191\n",
      "f1_score: 0.5652173913043478\n",
      "Accuracy: 0.8245614035087719\n"
     ]
    }
   ],
   "source": [
    "clf5 = LinearDiscriminantAnalysis()\n",
    "clf5.fit(X_train, y_train)\n",
    "y_pred = clf5.predict(X_val)\n",
    "print_metrics(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a21c234",
   "metadata": {},
   "source": [
    "# QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a48b7493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.09523809523809523\n",
      "f1_score: 0.17391304347826084\n",
      "Accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "clf6 = QuadraticDiscriminantAnalysis()\n",
    "clf6.fit(X_train, y_train)\n",
    "y_pred = clf6.predict(X_val)\n",
    "print_metrics(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b55a11",
   "metadata": {},
   "source": [
    "# ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1b59287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.35714285714285715\n",
      "Recall: 0.47619047619047616\n",
      "f1_score: 0.40816326530612246\n",
      "Accuracy: 0.7456140350877193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf7 = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf7.fit(X_train, y_train)\n",
    "y_pred = clf7.predict(X_val)\n",
    "print_metrics(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfbaa70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
